import base64
import json
import mimetypes
from typing import Dict, List, Literal, Union, Optional, Generator
import httpx
import boto3
from anthropic_bedrock import AnthropicBedrock
from pydantic_core._pydantic_core import ValidationError
from edenai_apis.features import ProviderInterface, TextInterface, ImageInterface
from edenai_apis.features.image.logo_detection.logo_detection_dataclass import (
    LogoDetectionDataClass,
    LogoItem,
    LogoBoundingPoly,
)
from edenai_apis.features.text import GenerationDataClass, SummarizeDataClass
from edenai_apis.features.text.chat.chat_dataclass import (
    StreamChat,
    ChatDataClass,
)
from edenai_apis.features.multimodal.chat.chat_dataclass import (
    ChatDataClass as ChatMultimodalDataClass,
    StreamChat as StreamChatMultimodal,
    ChatMessageDataClass as ChatMultimodalMessageDataClass,
)
from edenai_apis.loaders.data_loader import ProviderDataEnum
from edenai_apis.loaders.loaders import load_provider
from edenai_apis.utils.types import ResponseType
from edenai_apis.apis.amazon.helpers import handle_amazon_call
from edenai_apis.utils.exception import ProviderException
from edenai_apis.loaders.data_loader import load_info_file
from edenai_apis.apis.anthropic.prompts import LOGO_DETECTION_SYSTEM_PROMPT
from edenai_apis.llm_engine.llm_engine import LLMEngine


class AnthropicApi(ProviderInterface, TextInterface, ImageInterface):
    provider_name = "anthropic"

    def __init__(self, api_keys: Dict = {}) -> None:
        self.api_settings = load_provider(
            ProviderDataEnum.KEY, self.provider_name, api_keys=api_keys
        )
        self.bedrock = boto3.client(
            "bedrock-runtime",
            region_name=self.api_settings["region_name"],
            aws_access_key_id=self.api_settings["aws_access_key_id"],
            aws_secret_access_key=self.api_settings["aws_secret_access_key"],
        )
        self.llm_client = LLMEngine(
            provider_name=self.provider_name,
            provider_config={"api_key": self.api_settings.get("api_key")},
        )
        self.client = AnthropicBedrock()

    def __anthropic_request(self, request_body: str, model: str):
        # Headers for the HTTP request
        accept_header = "application/json"
        content_type_header = "application/json"

        # Parameters for the HTTP request
        request_params = {
            "body": request_body,
            "modelId": f"{self.provider_name}.{model}",
            "accept": accept_header,
            "contentType": content_type_header,
        }
        response = handle_amazon_call(self.bedrock.invoke_model, **request_params)
        response_body = json.loads(response.get("body").read())
        return response_body

    def __get_anthropic_version(self):
        """
        Retrieve the used version of anthropic
        """
        info = load_info_file(provider_name=self.provider_name)
        anthropic_version = info["multimodal"]["chat"]["version"]
        return anthropic_version

    def __calculate_usage(self, prompt: str, generated_text: str):
        """
        Calculate token usage based on the provided prompt and generated text.

        Args:
            prompt (str): The prompt provided to the language model.
            generated_text (str): The text generated by the language model.

        Returns:
            dict: A dictionary containing token usage details including total tokens,
                prompt tokens, and completion tokens.
        """
        try:
            prompt_tokens = self.client.count_tokens(prompt)
            completion_tokens = self.client.count_tokens(generated_text)
            return {
                "total_tokens": prompt_tokens + completion_tokens,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
            }
        except Exception:
            raise ProviderException("Client Error", status_code=500)

    def text__generation(
        self,
        text: str,
        temperature: float,
        max_tokens: int,
        model: str,
    ) -> ResponseType[GenerationDataClass]:
        prompt = f"\n\nHuman:{text}\n\nAssistant:"
        # Body of the HTTP request, containing text, maxTokens, and temperature
        request_body = json.dumps(
            {
                "prompt": prompt,
                "temperature": temperature,
                "max_tokens_to_sample": max_tokens,
            }
        )
        response = self.__anthropic_request(request_body=request_body, model=model)
        generated_text = response["completion"]
        response["usage"] = self.__calculate_usage(
            prompt=prompt, generated_text=generated_text
        )
        standardized_response = GenerationDataClass(generated_text=generated_text)

        return ResponseType[GenerationDataClass](
            original_response=response,
            standardized_response=standardized_response,
        )

    def text__summarize(
        self,
        text: str,
        output_sentences: int,
        language: str,
        model: Optional[str] = None,
    ) -> ResponseType[SummarizeDataClass]:
        messages = [
            {
                "role": "user",
                "content": f"Given the following text, please provide a concise summary of this text : {text}",
            },
            {
                "role": "assistant",
                "content": """Summary:""",
            },
        ]
        body = {
            "anthropic_version": self.__get_anthropic_version(),
            "max_tokens": 10000,
            "temperature": 0,
            "messages": messages,
        }

        request_body = json.dumps(body)

        original_response = self.__anthropic_request(
            request_body=request_body, model=model
        )

        # Calculate total usage
        original_response["usage"]["total_tokens"] = (
            original_response["usage"]["input_tokens"]
            + original_response["usage"]["output_tokens"]
        )

        result = original_response["content"][0]["text"]

        standardized_response = SummarizeDataClass(result=result)
        return ResponseType[SummarizeDataClass](
            original_response=original_response,
            standardized_response=standardized_response,
        )

    def text__chat(
        self,
        text: str,
        chatbot_global_action: Optional[str] = None,
        previous_history: Optional[List[Dict[str, str]]] = None,
        temperature: float = 0.0,
        max_tokens: int = 25,
        model: Optional[str] = None,
        stream: bool = False,
        available_tools: Optional[List[dict]] = None,
        tool_choice: Literal["auto", "required", "none"] = "auto",
        tool_results: Optional[List[dict]] = None,
    ) -> ResponseType[Union[ChatDataClass, StreamChat]]:
        response = self.llm_client.chat(
            text=text,
            previous_history=previous_history,
            chatbot_global_action=chatbot_global_action,
            temperature=temperature,
            max_tokens=max_tokens,
            model=model,
            stream=stream,
            available_tools=available_tools,
            tool_choice=tool_choice,
            tool_results=tool_results,
        )
        return response

    def multimodal__chat(
        self,
        messages: List[ChatMultimodalMessageDataClass],
        chatbot_global_action: Optional[str],
        temperature: float = 0,
        max_tokens: int = 25,
        model: Optional[str] = None,
        stop_sequences: Optional[List[str]] = None,
        top_k: Optional[int] = None,
        top_p: Optional[int] = None,
        stream: bool = False,
        provider_params: Optional[dict] = None,
        response_format=None,
    ) -> ResponseType[Union[ChatMultimodalDataClass, StreamChatMultimodal]]:
        response = self.llm_client.multimodal_chat(
            messages=messages,
            chatbot_global_action=chatbot_global_action,
            temperature=temperature,
            max_tokens=max_tokens,
            model=model,
            stop_sequences=stop_sequences,
            top_k=top_k,
            top_p=top_p,
            stream=stream,
            response_format=response_format,
        )
        return response

    def image__logo_detection(
        self, file: str, file_url: str = "", model: Optional[str] = None
    ) -> ResponseType[LogoDetectionDataClass]:
        mime_type = mimetypes.guess_type(file)[0]
        with open(file, "rb") as fstream:
            base64_data = base64.b64encode(fstream.read()).decode("utf-8")

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": mime_type,
                            "data": base64_data,
                        },
                    }
                ],
            }
        ]
        body = {
            "anthropic_version": self.__get_anthropic_version(),
            "max_tokens": 10000,
            "messages": messages,
            "system": LOGO_DETECTION_SYSTEM_PROMPT,
        }
        request_body = json.dumps(body)
        original_response = self.__anthropic_request(
            request_body=request_body, model=model
        )

        # Calculate total usage
        original_response["usage"]["total_tokens"] = (
            original_response["usage"]["input_tokens"]
            + original_response["usage"]["output_tokens"]
        )

        try:
            logos = json.loads(original_response["content"][0]["text"])
        except (KeyError, json.JSONDecodeError, ValidationError) as exc:
            raise ProviderException(
                "An error occurred while parsing the response."
            ) from exc

        items: List[LogoItem] = [
            LogoItem(
                description=logo,
                bounding_poly=LogoBoundingPoly(vertices=[]),
                score=None,
            )
            for logo in logos.get("items", [])
        ]

        return ResponseType[LogoDetectionDataClass](
            original_response=original_response,
            standardized_response=LogoDetectionDataClass(items=items),
        )
